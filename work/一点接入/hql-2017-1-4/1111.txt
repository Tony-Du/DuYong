数据抽取和上传ftp的脚本
 sudo su - hadoop -c  "sh /bigdata/etl/common/cpa_copy_hdfs_to_ftp/main.sh #flow.hdfs_path# #flow.local_path# #flow.file_name#"
 
 第一个：有效激活明细表
 hdfs_path  /user/hive/warehouse/app.db/cpa_active_device_detail_monthly/src_file_month=#flow.startDataTime#
 
local_path
/mnt/cpa/ftp/#tostring(#adddays(#addmonths(#todate(#flow.startDataTime#+'01', 'yyyyMMdd')#, 1)#,-1)# ,'yyyyMMdd')#/month

file_name
CPA_ACTIVE_DETAIL_#flow.startDataTime#.013
 
 
 
 第二个：有效
 hdfs_path
/user/hive/warehouse/app.db/cpa_settlement_detail_monthly/src_file_month=#flow.startDataTime#

local_path
/mnt/cpa/ftp/

file_name
CPA_DETAIL_#flow.startDataTime#.013
 
 第三个；主表
hdfs_path
/user/hive/warehouse/app.db/cpa_settlement_total_monthly/src_file_month=#flow.startDataTime#

local_path
/mnt/cpa/ftp/
file_name
CPA_MAIN_#flow.startDataTime#.013